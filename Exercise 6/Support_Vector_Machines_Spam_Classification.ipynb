{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In this exercise, you will use SVMs to build your own spam filter.\n",
    "\n",
    "You will be training a classifier to classify whether a given email, $x$, is spam ($y = 1$) or non-spam ($y = 0$). In particular, you need to convert each email into a feature vector $x \\in \\mathbb{R}^n$.\n",
    "\n",
    "The dataset included for this exercise is based on a subset of the [SpamAssassin Public Corpus](http://spamassassin.apache.org/old/publiccorpus/). For the purpose of this exercise, you will only be using the body of the email (excluding the email headers).\n",
    "\n",
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Import regular expressions to process emails\n",
    "import re\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# Will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Scikit-Learn contains the svm library, which contains built-in classes for different SVM algorithms. \n",
    "# Since we are going to perform a classification task, we will use the support vector classifier class, \n",
    "# which is written as SVC in the Scikit-Learn's svm library\n",
    "from sklearn import svm\n",
    "\n",
    "# This porter stemmer more accurately duplicates the porter stemmer used in the OCTAVE assignment code\n",
    "import nltk, nltk.stem.porter\n",
    "\n",
    "# Tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Emails\n",
    "\n",
    "I will first start by taking a look at one example from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'Data/emailSample1.txt'\n",
    "\n",
    "with open(filename) as file:\n",
    "    example = file.read()\n",
    "    \n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many emails contain similar types of entities (e.g. numbers, URLs, email addresses, etc.), the specific entities (e.g. the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often used in processing emails is to 'normalize' these values so that all URLs are treated the same, all numbers are treated the same, etc. For example, to 'normalize' URLs in an email, we could replace each URL in the email with the unique string 'httpaddr' to indicate that a URL was present.\n",
    "\n",
    "This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This improves the performance of a spam classifier because, since spammers often randomize the URLs, the odds of seeing any particular URL again in a new piece of spam is very small. \n",
    "\n",
    "For this purpose I will use the function `processEmail` which will implement the following email preprocessing and normalization steps:\n",
    "\n",
    "- **Lower-casing**: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate)\n",
    "\n",
    "- **Stripping HTML**: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains\n",
    "\n",
    "- **Normalizing URLs**: All URLs are replaced with the text 'httpaddr'\n",
    "\n",
    "- **Normalizing Email Addresses**:  All email addresses are replaced with the text 'emailaddr'\n",
    "\n",
    "- **Normalizing Numbers**: All numbers are replaced with the text 'number'\n",
    "\n",
    "- **Normalizing Dollar Sign**: All dollar signs ($) are replaced with the text 'dollar'\n",
    "\n",
    "- **Word Stemming**: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”\n",
    "\n",
    "- **Removal of non-words**: Non-words and punctuation are removed. All white spaces (tabs, newlines, spaces) are trimmed to a single space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(email):\n",
    "    # Find the Headers (the header and the rest of the content are usually separated by a double line break (\\n\\n))\n",
    "    # and remove them\n",
    "    # Uncomment the following lines if I am working with raw emails with the full headers\n",
    "    # hdrstart = email_contents.find(chr(10) + chr(10))\n",
    "    # email_contents = email_contents[hdrstart:]\n",
    "\n",
    "    # Turn every word in the email to lowercase\n",
    "    email = email.lower()\n",
    "    \n",
    "    # Strip HTML tags - strings that start with '<' and end with '>' where the content inside does not\n",
    "    # contain '<' or '>' are replaced with a whitespace\n",
    "    email =re.compile('<[^<>]+>').sub(' ', email)\n",
    "\n",
    "    # Any number gets replaced by the string 'number'\n",
    "    # Spaces in the beginning and end of the string ' number ' separate it from any other word\n",
    "    email = re.compile('[0-9]+').sub(' number ', email)\n",
    "\n",
    "    # URLS - anything starting with http or https:// is replaced with 'httpaddr'\n",
    "    email= re.compile('(http|https)://[^\\s]*').sub(' httpaddr ', email)\n",
    "\n",
    "    # Email Addresses - strings with '@' in the middle are replaced with 'emailaddr'\n",
    "    email = re.compile('[^\\s]+@[^\\s]+').sub(' emailaddr ', email)\n",
    "    \n",
    "    # $ sign - Replaces dollar signs with the string 'dollar'\n",
    "    email = re.compile('[$]+').sub(' dollar ', email)\n",
    "    \n",
    "    # All punctuation is removed\n",
    "    email = re.split('[ @$/#.-:&*+=\\[\\]?!(){},''\">_<;%\\n\\r]', email)\n",
    "\n",
    "    # Remove any empty word string\n",
    "    email = [word for word in email if len(word) > 0]\n",
    "    \n",
    "    # Stem the email contents word by word\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    processed_email = []\n",
    "    for word in email:\n",
    "        # Remove any remaining non alphanumeric characters in word\n",
    "        word = re.compile('[^a-zA-Z0-9]').sub('', word).strip()\n",
    "        word = stemmer.stem(word)\n",
    "        processed_email.append(word)\n",
    "        \n",
    "    return processed_email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use this function to preprocess the example from the dataset to see what the email looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anyon know how much it cost to host a web portal well it depend on how mani visitor your expect thi can be anywher from less than number buck a month to a coupl of dollar number you should checkout httpaddr or perhap amazon ec number if your run someth big to unsubscrib yourself from thi mail list send an email to emailaddr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(processEmail(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary list\n",
    "\n",
    "After preprocessing the emails, I have a list of words for each email. The next step is to choose which words I would like to use in the classifier and which I would want to leave out.\n",
    "\n",
    "For this exercise, I was given a vocabulary list in the file `vocab.txt` which was selected by choosing all words which occur at least 100 times in the spam corpus, resulting in a list of 1899 words. This small list is used since words that occur rarely in the training set are only in a few emails and so they might cause the model to overfit the training set.\n",
    "In practice, a vocabulary list with about 10,000 to 50,000 words is often used.\n",
    "\n",
    "Given this vocabulary list, I can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list.\n",
    "\n",
    "To perform this mapping, I will write the function `indexInVocabList` to perform this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/vocab.txt') as file:\n",
    "    vocab = file.read()\n",
    "\n",
    "# These lines of code are to turn the data in the 'vocab.txt' file into a list that contains only the words\n",
    "no_space = re.compile('\\s').sub('', vocab) # Remove all whitespace\n",
    "comma_separated = re.compile('\\d+').sub(',', no_space) # Turn numbers into a comma so I can split the data into an array\n",
    "vocabList = comma_separated.split(',')[1:] # First element is a whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 915, 793, 1076, 882, 369, 1698, 789, 1821, 1830, 882, 430, 1170, 793, 1001, 1894, 591, 1675, 237, 161, 88, 687, 944, 1662, 1119, 1061, 1698, 374, 1161, 476, 1119, 1892, 1509, 798, 1181, 1236, 511, 1119, 809, 1894, 1439, 1546, 180, 1698, 1757, 1895, 687, 1675, 991, 960, 1476, 70, 529, 1698, 530]\n"
     ]
    }
   ],
   "source": [
    "def indexInVocabList(email):\n",
    "    processed_email = processEmail(email) # Preprocess email\n",
    "    word_indices = []\n",
    "   \n",
    "    for word in processed_email:\n",
    "        # Look up the word in the vocabulary list and add to word_indices if found\n",
    "        try:\n",
    "            index = vocabList.index(word)\n",
    "            word_indices.append(index)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return word_indices\n",
    "        \n",
    "# List of word indices for the example\n",
    "print(indexInVocabList(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting features from emails\n",
    "\n",
    "Now I will implement the feature extraction using the function `emailFeatures` that converts each email into a vector of features. I will use the vocabulary list as the features vector for each email and the values of the vector will be $x_i \\in \\{0, 1\\}$. If the $i^{th}$ word of the vocabulary list is in the email then the feature value will be $x_i = 1$ and if the $i^{th}$ word is not present in the email then the feature value will be $x_i = 0$.\n",
    "\n",
    "The features for an email would look like this:\n",
    "\n",
    "$$ x = \\begin{bmatrix} \n",
    "0 & \\dots & 1 & 0 & \\dots & 1 & 0 & \\dots & 0 \n",
    "\\end{bmatrix}^T \\in \\mathbb{R}^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    # Total number of words in the vocabulary list\n",
    "    n = len(vocabList)\n",
    "\n",
    "    # Initializing variable to be used below\n",
    "    x = np.zeros(n)\n",
    "    \n",
    "    # Each word in the vocabulary list has an index and I turned the email into a list of word indices\n",
    "    # If index is in the range then that means that the word from the email exists in the vocabulary list and so it gets a\n",
    "    # value of 1 and if it doesn't exist then it gets a value of 0\n",
    "    for i in range(n):\n",
    "        if i in word_indices:\n",
    "            x[i] = 1\n",
    "        else:\n",
    "            x[i] = 0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 45\n"
     ]
    }
   ],
   "source": [
    "word_indices = indexInVocabList(example)\n",
    "features = emailFeatures(word_indices)\n",
    "\n",
    "# Print Stats\n",
    "print('Length of feature vector: %d' % len(features))\n",
    "print('Number of non-zero entries: %d' % sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I know how to get a dataset that I can use for spam classification from a collection of emails.\n",
    "\n",
    "#### Training SVM for Spam Classification\n",
    "\n",
    "For this part of the exercise, I will load a preprocessed training set from the file `spamTrain.mat` and a preprocessed test set from the file `spamTrain.mat` that will be used to train a SVM classifier. The file `spamTrain.mat` contains 4000 training examples of spam and non-spam email, while `spamTest.mat` contains 1000 test examples. Each original email was processed using the `processEmail`, `indexInVocabList` and `emailFeatures` functions and converted into a vector $x^{(i)} \\in \\mathbb{R}^{1899}$.\n",
    "\n",
    "Now I will train a linear SVM to classify between spam ($y = 1$) and non-spam ($y = 0$) emails. Once the training completes, you should see that the classifier gets a training accuracy of about 99.8% and a test accuracy of about 98.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.82%\n",
      "Test Accuracy: 98.9%\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "data = loadmat('Data/spamTrain.mat')\n",
    "x, y = data['X'].astype(float), data['y'][:, 0]\n",
    "\n",
    "# Load the test dataset\n",
    "data = loadmat('Data/spamTest.mat')\n",
    "x_test, y_test = data['Xtest'].astype(float), data['ytest'][:, 0]\n",
    "\n",
    "# SVM with linear kernel\n",
    "linear_svm = svm.SVC(C=0.1, kernel='linear')\n",
    "\n",
    "# Train model using the training dataset\n",
    "linear_svm.fit(x, y)\n",
    "\n",
    "predictions_train = linear_svm.predict(x)\n",
    "predictions_test = linear_svm.predict(x_test)\n",
    "\n",
    "print(f'Training Accuracy: {round((np.mean(predictions_train == y) * 100), 2)}%')\n",
    "print(f'Test Accuracy: {round((np.mean(predictions_test == y_test) * 100), 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top predictors for spam\n",
    "\n",
    "To better understand how the spam classifier works, I can inspect the parameters of the linear SVM model to see which words the classifier thinks are the most predictive of spam. To do that I will find the parameters with the largest positive values in the classifier and display the corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most important words to classify a spam e-mail are:\n",
      "['our', 'click', 'remov', 'guarante', 'visit', 'basenumb', 'dollar', 'will', 'price', 'pleas', 'most', 'nbsp', 'lo', 'ga', 'hour']\n",
      "\n",
      "The 15 least important words to classify a spam e-mail are:\n",
      "['httpaddr', 'tom', 'yahoo', 'razor', 'author', 'until', 'user', 'numbertnumb', 'rpm', 'list', 'date', 'wrote', 'url', 'the', 'spamassassin']\n"
     ]
    }
   ],
   "source": [
    "# Sort indices from most important to least-important (highest to lowest weight)\n",
    "sorted_indices = np.argsort(linear_svm.coef_, axis=None)[::-1]\n",
    "\n",
    "print(\"The 15 most important words to classify a spam e-mail are:\")\n",
    "print([vocabList[x] for x in sorted_indices[:15]])\n",
    "print('')\n",
    "print(\"The 15 least important words to classify a spam e-mail are:\")\n",
    "print([vocabList[x] for x in sorted_indices[-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an email contains words such as “click”, “remov”, “visit” or any other of the words above (the top predictors of spam) then it is likely going to be classified as spam.\n",
    "\n",
    "#### Spam classification on new emails\n",
    "\n",
    "Now that I have trained a spam classifier, I will try it out on a few new emails to see if it classifies them properly. In the Data folder are included two email examples (`emailSample1.txt` and `emailSample2.txt`) and two spam examples (`spamSample1.txt` and `spamSample2.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification: Not spam\n"
     ]
    }
   ],
   "source": [
    "with open('Data/emailSample1.txt') as file:\n",
    "    email_1 = file.read()\n",
    "\n",
    "word_indices_1 = indexInVocabList(email_1)\n",
    "features_1 = emailFeatures(word_indices_1)\n",
    "prediction_1 = linear_svm.predict(features_1.reshape(1, -1))\n",
    "\n",
    "print('Spam Classification: %s' % ('Spam' if prediction_1 else 'Not spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification: Not spam\n"
     ]
    }
   ],
   "source": [
    "with open('Data/emailSample2.txt') as file:\n",
    "    email_2 = file.read()\n",
    "\n",
    "word_indices_2 = indexInVocabList(email_2)\n",
    "features_2 = emailFeatures(word_indices_2)\n",
    "prediction_2 = linear_svm.predict(features_2.reshape(1, -1))\n",
    "\n",
    "print('Spam Classification: %s' % ('Spam' if prediction_2 else 'Not spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification: Spam\n"
     ]
    }
   ],
   "source": [
    "with open('Data/spamSample1.txt') as file:\n",
    "    email_3 = file.read()\n",
    "\n",
    "word_indices_3 = indexInVocabList(email_3)\n",
    "features_3 = emailFeatures(word_indices_3)\n",
    "prediction_3 = linear_svm.predict(features_3.reshape(1, -1))\n",
    "\n",
    "print('Spam Classification: %s' % ('Spam' if prediction_3 else 'Not spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classification: Spam\n"
     ]
    }
   ],
   "source": [
    "with open('Data/spamSample2.txt') as file:\n",
    "    email_4 = file.read()\n",
    "\n",
    "word_indices_4 = indexInVocabList(email_4)\n",
    "features_4 = emailFeatures(word_indices_4)\n",
    "prediction_4 = linear_svm.predict(features_4.reshape(1, -1))\n",
    "\n",
    "print('Spam Classification: %s' % ('Spam' if prediction_4 else 'Not spam'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
